# kafka-partioning

This working example domanstrates CustomPartitioning implementation to dispatch events generated by producer to different partitions of topic in round robin manner. Number of partitions are set while creating the kafka topic. While creating KafkaProducer, CustomPartitioner class which implements basic distribution logic as configuration parameter(PARTITIONER_CLASS_CONFIG) is registered. In our example we use Integer data type as key. We should pay attention to KEY_DESERIALIZER_CLASS_CONFIG and VALUE_DESERIALIZER_CLASS_CONFIG data types, they should be matching with data type of Key and Value pairs.

## CustomPartitioner Class

```java
import java.util.Map;
import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;

public class CustomPartitioner implements Partitioner {

  private static final int PARTITION_COUNT=3;

  @Override
  public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
    Integer keyInt= (Integer)key;
    System.out.println((keyInt % PARTITION_COUNT));
    return keyInt % PARTITION_COUNT;
  }
  @Override
  public void close() {
  }

  @Override
  public void configure(Map<String, ?> configs) {

  }
}
```
## Monolith Tester Class

```java
import java.time.Duration;
import java.time.temporal.ChronoUnit;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Properties;
import java.util.concurrent.ExecutionException;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.IntegerDeserializer;
import org.apache.kafka.common.serialization.IntegerSerializer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

public class Tester {

  public static String KAFKA_BROKERS = "localhost:9092";
  public static Integer MESSAGE_COUNT = 1000;
  public static String CLIENT_ID = "client1";
  public static String TOPIC_NAME = "part-demo";
  public static String GROUP_ID_CONFIG = "consumerGroup10";
  public static Integer MAX_NO_MESSAGE_FOUND_COUNT = 100;
  public static String OFFSET_RESET_LATEST = "latest";
  public static String OFFSET_RESET_EARLIER = "earliest";
  public static Integer MAX_POLL_RECORDS = 1;

  public static Producer<Integer, String> createProducer() {
    Properties props = new Properties();
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BROKERS);
    props.put(ProducerConfig.CLIENT_ID_CONFIG, CLIENT_ID);
    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class.getName());
    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, CustomPartitioner.class.getName());
    return new KafkaProducer<>(props);
  }

  private static Consumer<Integer, String> createConsumer() {
    final Properties props = new Properties();
    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BROKERS);
    props.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID_CONFIG);
    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());
    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, MAX_POLL_RECORDS);
    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, OFFSET_RESET_EARLIER);
    final Consumer<Integer, String> myConsumer = new KafkaConsumer<>(props);
    myConsumer.subscribe(Collections.singletonList(TOPIC_NAME));
    return myConsumer;
  }

  public static void main(String[] argv) throws Exception {
    Runnable runnable = () -> {
      while (true) {
        try {
          startProducer();
        } catch (Exception e) {
          e.printStackTrace();
        }
      }
    };
    Thread t = new Thread(runnable);
    t.start();
    Thread.sleep(1000);
    startConsumer();
  }

  private static void startConsumer() {
    Consumer<Integer, String> myConsumer = createConsumer();

    myConsumer.subscribe(Arrays.asList(TOPIC_NAME), new ConsumerRebalanceListener() {
      public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
        System.out.printf("%s topic-partitions are revoked from this consumer\n", Arrays.toString(partitions.toArray()));
      }

      public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
        System.out.printf("%s topic-partitions are assigned to this consumer\n", Arrays.toString(partitions.toArray()));
      }
    });

    int noMessageToFetch = 0;
    while (true) {
      final ConsumerRecords<Integer, String> consumerRecords = myConsumer.poll(Duration.of(1000, ChronoUnit.MILLIS));
      if (consumerRecords.count() == 0) {
        noMessageToFetch++;
        if (noMessageToFetch > MAX_NO_MESSAGE_FOUND_COUNT) {
          break;
        } else {
          continue;
        }
      }

      consumerRecords.forEach(record -> {
        System.out.println("Record Key " + record.key());
        System.out.println("Record value " + record.value());
        System.out.println("Record partition " + record.partition());
        System.out.println("Record offset " + record.offset());
      });
      myConsumer.commitAsync();
    }
    myConsumer.close();
  }

  private static void startProducer() {
    Producer<Integer, String> producer = createProducer();

    for (int key = 0; key < MESSAGE_COUNT; key++) {
      final ProducerRecord<Integer, String> record = new ProducerRecord<>(TOPIC_NAME, key,
          "This is record " + key);
      try {
        RecordMetadata metadata = producer.send(record).get();
        System.out.println("Record sent with key " + key + " to partition " + metadata.partition()
            + " with offset " + metadata.offset());
        Thread.sleep(500);
      } catch (ExecutionException e) {
        System.out.println("Error in sending record");
        System.out.println(e);
      } catch (InterruptedException e) {
        System.out.println("Error in sending record");
        System.out.println(e);
      }
    }

  }
}
```
### Starting up Kafka and Creating Topic

Make sure that you have installed Confluent and Confluent CLI for spinning up Kafka environment easily on local development environment.
```
$ export CONFLUENT_HOME=/Users/kemalatik/etc/confluent-5.3.2
$ export CONFLUENT_CLI=/Users/kemalatik/etc/confluent-cli
$ export PATH="${CONFLUENT_HOME}/bin:$PATH"
$ export PATH="${CONFLUENT_CLI}/bin:$PATH"
$ ./confluent local start kafka
Updates are available for confluent. To install them, please run:
$ confluent update

    The local commands are intended for a single-node development environment
    only, NOT for production usage. https://docs.confluent.io/current/cli/index.html

Using CONFLUENT_CURRENT: /var/folders/9d/8tj1tdgx4tvbq4rmr4xdtfkr0000gp/T/confluent.B25OyaXW
Starting zookeeper
zookeeper is [UP]
Starting kafka
kafka is [UP]
```
Creating topic with 3 partitions. 
```
$ kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic part-demo
Created topic part-demo.

```

